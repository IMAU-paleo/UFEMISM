MODULE petsc_module

  ! Contains routines that use the PETSc matrix solvers
  !
  ! Convention: xx = Fortran, x = PETSc

#include <petsc/finclude/petscksp.h>
  USE petscksp
  USE mpi
  USE parallel_module,                 ONLY: par, sync, cerr, ierr, partition_list, &
                                             allocate_shared_int_0D, allocate_shared_dp_0D, &
                                             allocate_shared_int_1D, allocate_shared_dp_1D, &
                                             allocate_shared_int_2D, allocate_shared_dp_2D, &
                                             allocate_shared_int_3D, allocate_shared_dp_3D, &
                                             deallocate_shared
  USE configuration_module,            ONLY: dp, C, routine_path, init_routine, finalise_routine, crash, warning
  USE data_types_module,               ONLY: type_sparse_matrix_CSR_dp

  IMPLICIT NONE

 ! Some interfaces are not autogenerated, provide them here to avoid implicit calls
 interface
   subroutine VecDestroy( vec, err)
     import
     type(tVec) :: vec
     integer    :: err
   end subroutine
   subroutine KspDestroy( ksp, err)
     import
     type(tKSP) :: ksp
     integer    :: err
   end subroutine
   subroutine MatDestroy( mat, err)
     import
     type(tMat) :: mat
     integer    :: err
   end subroutine
 end interface

  INTEGER :: perr    ! Error flag for PETSc routines

CONTAINS

! == Solve a square CSR matrix equation with PETSc
  SUBROUTINE solve_matrix_equation_CSR_PETSc( A_CSR, bb, xx, rtol, abstol)

    IMPLICIT NONE

    ! In/output variables:
    TYPE(type_sparse_matrix_CSR_dp),     INTENT(IN)    :: A_CSR
    REAL(dp), DIMENSION(:    ),          INTENT(IN)    :: bb
    REAL(dp), DIMENSION(:    ),          INTENT(INOUT) :: xx
    REAL(dp),                            INTENT(IN)    :: rtol, abstol

    ! Local variables
    CHARACTER(LEN=256), PARAMETER                      :: routine_name = 'solve_matrix_equation_CSR_PETSc'
    TYPE(tMat)                                         :: A

    ! Add routine to path
    CALL init_routine( routine_name)

    ! Convert matrix to PETSc format
    CALL mat_CSR2petsc( A_CSR, A)

    ! Solve the PETSC matrix equation
    CALL solve_matrix_equation_PETSc( A, bb, xx, rtol, abstol)

    ! Clean up after yourself
    CALL MatDestroy( A, perr)

    ! Finalise routine path
    CALL finalise_routine( routine_name)

  END SUBROUTINE solve_matrix_equation_CSR_PETSc

  SUBROUTINE solve_matrix_equation_PETSc( A, bb, xx, rtol, abstol)
    ! Solve the matrix equation using a Krylov solver from PETSc

    IMPLICIT NONE

    ! In/output variables:
    TYPE(tMat),                          INTENT(IN)    :: A
    REAL(dp), DIMENSION(:    ),          INTENT(IN)    :: bb
    REAL(dp), DIMENSION(:    ),          INTENT(INOUT) :: xx
    REAL(dp),                            INTENT(IN)    :: rtol, abstol

    ! Local variables:
    CHARACTER(LEN=256), PARAMETER                      :: routine_name = 'solve_matrix_equation_PETSc'
    INTEGER                                            :: m, n
    TYPE(tVec)                                         :: b
    TYPE(tVec)                                         :: x
    TYPE(tKSP)                                         :: KSP_solver
    INTEGER                                            :: its

    ! Add routine to path
    CALL init_routine( routine_name)

    ! Safety
    CALL MatGetSize( A, m, n, perr)
    IF (m /= n .OR. m /= SIZE( xx,1) .OR. m /= SIZE( bb,1)) THEN
      CALL crash('matrix and vector sizes dont match!')
    END IF

  ! == Set up right-hand side and solution vectors as PETSc data structures
  ! =======================================================================

    CALL vec_double2petsc( xx, x)
    CALL vec_double2petsc( bb, b)

  ! Set up the solver
  ! =================

    ! Set up the KSP solver
    CALL KSPcreate( PETSC_COMM_WORLD, KSP_solver, perr)

    ! Set operators. Here the matrix that defines the linear system
    ! also serves as the preconditioning matrix.
    CALL KSPSetOperators( KSP_solver, A, A, perr)

    ! Iterative solver tolerances
    CALL KSPSetTolerances( KSP_solver, rtol, abstol, PETSC_DEFAULT_REAL, PETSC_DEFAULT_INTEGER, perr)

    ! Set runtime options, e.g.,
    !     -ksp_type <type> -pc_type <type> -ksp_monitor -ksp_rtol <rtol>
    ! These options will override those specified above as long as
    ! KSPSetFromOptions() is called _after_ any other customization routines.
    CALL KSPSetFromOptions( KSP_solver, perr)

  ! == Solve Ax=b
  ! =============

    ! Solve the linear system
    CALL KSPSolve( KSP_solver, b, x, perr)

    ! Find out how many iterations it took
    CALL KSPGetIterationNumber( KSP_solver, its, perr)
    !IF (par%master) WRITE(0,*) '   PETSc solved Ax=b in ', its, ' iterations'

    ! Get the solution back to the native UFEMISM storage structure
    CALL vec_petsc2double( x, xx)

    ! Clean up after yourself
    CALL KSPDestroy( KSP_solver, perr)
    CALL VecDestroy( x, perr)
    CALL VecDestroy( b, perr)

    ! Finalise routine path
    CALL finalise_routine( routine_name)

  END SUBROUTINE solve_matrix_equation_PETSc

! == Conversion between 1-D Fortran double-precision arrays and PETSc parallel vectors
  SUBROUTINE vec_double2petsc( xx, x)
    ! Convert a regular 1-D Fortran double-precision array to a PETSc parallel vector

    IMPLICIT NONE

    ! In- and output variables:
    REAL(dp), DIMENSION(:    ),          INTENT(IN)    :: xx
    TYPE(tVec),                          INTENT(INOUT) :: x

    ! Local variables:
    CHARACTER(LEN=256), PARAMETER                      :: routine_name = 'vec_double2petsc'
    TYPE(PetscInt)                                     :: istart,iend,i

    ! Add routine to path
    CALL init_routine( routine_name)

    ! Create parallel vector
    CALL VecCreate( PETSC_COMM_WORLD, x, perr)
    CALL VecSetSizes( x, PETSC_DECIDE, SIZE( xx,1), perr)
    CALL VecSetFromOptions( x, perr)

    ! Get parallelisation domains ("ownership ranges")
    CALL VecGetOwnershipRange( x, istart, iend, perr)

    ! Fill in vector values
    DO i = istart+1,iend ! +1 because PETSc indexes from 0
      CALL VecSetValues( x, 1, i-1, xx( i), INSERT_VALUES, perr)
    END DO
    CALL sync

    ! Assemble vectors, using the 2-step process:
    !   VecAssemblyBegin(), VecAssemblyEnd()
    ! Computations can be done while messages are in transition
    ! by placing code between these two statements.

    CALL VecAssemblyBegin( x, perr)
    CALL VecAssemblyEnd(   x, perr)

    ! Finalise routine path
    CALL finalise_routine( routine_name)

  END SUBROUTINE vec_double2petsc

  SUBROUTINE vec_petsc2double( x, xx)
    ! Convert a PETSc parallel vector to a regular 1-D Fortran double-precision array

    IMPLICIT NONE

    ! In- and output variables:
    TYPE(tVec),                          INTENT(IN)    :: x
    REAL(dp), DIMENSION(:    ),          INTENT(OUT)   :: xx

    ! Local variables:
    CHARACTER(LEN=256), PARAMETER                      :: routine_name = 'vec_petsc2double'
    TYPE(PetscInt)                                     :: istart,iend,i,n
    TYPE(PetscInt),    DIMENSION(1)                    :: ix
    TYPE(PetscScalar), DIMENSION(1)                    :: v

    ! Add routine to path
    CALL init_routine( routine_name)

    ! Safety
    CALL VecGetSize( x, n, perr)
    IF (n /= SIZE( xx,1)) THEN
      CALL crash('Fortran and PETSc vector sizes dont match!')
    END IF

    ! Get parallelisation domains ("ownership ranges")
    CALL VecGetOwnershipRange( x, istart, iend, perr)

    ! Get values
    DO i = istart+1,iend
      ix(1) = i-1
      CALL VecGetValues( x, 1, ix, v, perr)
      xx( i) = v(1)
    END DO
    CALL sync

    ! Finalise routine path
    CALL finalise_routine( routine_name)

  END SUBROUTINE vec_petsc2double

  SUBROUTINE mat_petsc2CSR( A, A_CSR)
    ! Convert a PETSC parallel matrix to a CSR-format matrix in regular Fortran arrays

    IMPLICIT NONE

    ! In/output variables:
    TYPE(tMat),                          INTENT(IN)    :: A
    TYPE(type_sparse_matrix_CSR_dp),     INTENT(OUT)   :: A_CSR

    ! Local variables:
    CHARACTER(LEN=256), PARAMETER                      :: routine_name = 'mat_petsc2CSR'
    INTEGER                                            :: m, n, istart, iend, i
    INTEGER                                            :: ncols, nnz
    INTEGER,  DIMENSION(:    ), ALLOCATABLE            :: cols
    REAL(dp), DIMENSION(:    ), ALLOCATABLE            :: vals
    INTEGER,  DIMENSION(:    ), POINTER                ::  nnz_rows
    INTEGER                                            :: wnnz_rows
    INTEGER                                            :: k1, k2

    ! Add routine to path
    CALL init_routine( routine_name)

    ! First get the number of rows and columns
    CALL MatGetSize( A, m, n, perr)

    ! Find number of non-zeros per row
    CALL allocate_shared_int_1D( m, nnz_rows, wnnz_rows)

    CALL MatGetOwnershipRange( A, istart, iend, perr)

    ALLOCATE( cols( n))
    ALLOCATE( vals( n))

    DO i = istart+1, iend ! +1 because PETSc indexes from 0
      CALL MatGetRow( A, i-1, ncols, cols, vals, perr)
      nnz_rows( i) = ncols
      CALL MatRestoreRow( A, i-1, ncols, cols, vals, perr)
    END DO
    CALL sync

    ! Find the number of non-zeros
    nnz = SUM( nnz_rows)

    ! Allocate memory for the CSR matrix
    CALL allocate_shared_int_0D( A_CSR%m      , A_CSR%wm      )
    CALL allocate_shared_int_0D( A_CSR%n      , A_CSR%wn      )
    CALL allocate_shared_int_0D( A_CSR%nnz_max, A_CSR%wnnz_max)
    CALL allocate_shared_int_0D( A_CSR%nnz    , A_CSR%wnnz    )

    IF (par%master) THEN
      A_CSR%m       = m
      A_CSR%n       = n
      A_CSR%nnz_max = nnz
      A_CSR%nnz     = nnz
    END IF
    CALL sync

    CALL allocate_shared_int_1D( A_CSR%m+1, A_CSR%ptr  , A_CSR%wptr  )
    CALL allocate_shared_int_1D( A_CSR%nnz, A_CSR%index, A_CSR%windex)
    CALL allocate_shared_dp_1D(  A_CSR%nnz, A_CSR%val  , A_CSR%wval  )

    ! Fill in the ptr array
    IF (par%master) THEN
      A_CSR%ptr( 1) = 1
      DO i = 2, m+1
        A_CSR%ptr( i) = A_CSR%ptr( i-1) + nnz_rows( i-1)
      END DO
    END IF
    CALL sync

    ! Copy data from the PETSc matrix to the CSR arrays
    DO i = istart+1, iend ! +1 because PETSc indexes from 0
      k1 = A_CSR%ptr( i)
      k2 = A_CSR%ptr( i+1) - 1
      CALL MatGetRow( A, i-1, ncols, cols, vals, perr)
      A_CSR%index( k1:k2) = cols( 1:ncols)+1
      A_CSR%val(   k1:k2) = vals( 1:ncols)
      CALL MatRestoreRow( A, i-1, ncols, cols, vals, perr)
    END DO
    CALL sync

    ! Clean up after yourself
    CALL deallocate_shared( wnnz_rows)

    ! Finalise routine path
    CALL finalise_routine( routine_name, n_extra_windows_expected = 7)

  END SUBROUTINE mat_petsc2CSR

  SUBROUTINE mat_CSR2petsc( A_CSR, A)
    ! Convert a CSR-format matrix in regular Fortran arrays to a PETSC parallel matrix
    !
    ! NOTE: the PETSc documentation seems to advise against using the MatCreateMPIAIJWithArrays
    !       routine used here. However, for the advised way of using MatSetValues with preallocation
    !       I've not been able to find a way that is fast enough to be useful without having to
    !       preallocate -WAY- too much memory. Especially for the remapping matrices, which
    !       can have hundreds or even thousands of non-zero elements per row, this can make the
    !       model run hella slow, whereas the current solution seems to work perfectly. So there you go.

    IMPLICIT NONE

    ! In/output variables:
    TYPE(type_sparse_matrix_CSR_dp),     INTENT(IN)    :: A_CSR
    TYPE(tMat),                          INTENT(OUT)   :: A

    ! Local variables:
    CHARACTER(LEN=256), PARAMETER                      :: routine_name = 'mat_CSR2petsc'
    INTEGER                                            :: i1, i2, nrows_proc, nrows_scan, i, k1, k2, nnz_row, nnz_proc, ii, k, kk
    INTEGER,  DIMENSION(:    ), ALLOCATABLE            :: ptr_proc, index_proc
    REAL(dp), DIMENSION(:    ), ALLOCATABLE            :: val_proc

    ! Add routine to path
    CALL init_routine( routine_name)

    ! Determine process domains
    ! NOTE: slightly different from how it's done in partition_list, this is needed
    !       because otherwise PETSc will occasionally throw errors because the
    !       process domains are different from what it expects.
    nrows_proc = PETSC_DECIDE
    CALL PetscSplitOwnership( PETSC_COMM_WORLD, nrows_proc, A_CSR%m, perr)
    CALL MPI_Scan( nrows_proc, nrows_scan, 1, MPI_INTEGER, MPI_SUM, PETSC_COMM_WORLD, ierr)
    i1 = nrows_scan + 1 - nrows_proc
    i2 = i1 + nrows_proc - 1

    ! Determine number of non-zeros for this process
    nnz_proc = 0
    DO i = i1, i2
      k1 = A_CSR%ptr( i)
      k2 = A_CSR%ptr( i+1) - 1
      nnz_row = k2 + 1 - k1
      nnz_proc = nnz_proc + nnz_row
    END DO
    CALL sync

    ! Allocate memory for local CSR-submatrix
    ALLOCATE( ptr_proc(   0:nrows_proc    ))
    ALLOCATE( index_proc( 0:nnz_proc   - 1))
    ALLOCATE( val_proc(   0:nnz_proc   - 1))

    ! Copy matrix data
    DO i = i1, i2

      ! ptr
      ii = i - i1
      ptr_proc( ii) = A_CSR%ptr( i) - A_CSR%ptr( i1)

      ! index and val
      k1 = A_CSR%ptr( i)
      k2 = A_CSR%ptr( i+1) - 1
      DO k = k1, k2
        kk = k - A_CSR%ptr( i1)
        index_proc( kk) = A_CSR%index( k) - 1
        val_proc(   kk) = A_CSR%val(   k)
      END DO

    END DO
    ! Last row
    ptr_proc( nrows_proc) = A_CSR%ptr( i2+1) - A_CSR%ptr( i1)
    CALL sync

    ! Create PETSc matrix
    CALL MatCreateMPIAIJWithArrays( PETSC_COMM_WORLD, nrows_proc, PETSC_DECIDE, PETSC_DETERMINE, A_CSR%n, ptr_proc, index_proc, val_proc, A, perr)

    ! Assemble matrix and vectors, using the 2-step process:
    !   MatAssemblyBegin(), MatAssemblyEnd()
    ! Computations can be done while messages are in transition
    ! by placing code between these two statements.

    CALL MatAssemblyBegin( A, MAT_FINAL_ASSEMBLY, perr)
    CALL MatAssemblyEnd(   A, MAT_FINAL_ASSEMBLY, perr)

    ! Clean up after yourself
    DEALLOCATE( ptr_proc  )
    DEALLOCATE( index_proc)
    DEALLOCATE( val_proc  )

    ! Finalise routine path
    CALL finalise_routine( routine_name)

  END SUBROUTINE mat_CSR2petsc

  SUBROUTINE double2petscmat( x, A)
    ! Convert a regular 1-D Fortran double-precision array to an identity matrix in PETSc format

    IMPLICIT NONE

    ! In- and output variables:
    REAL(dp), DIMENSION(:    ),          INTENT(IN)    :: x
    TYPE(tMat),                          INTENT(OUT)   :: A

    ! Local variables:
    INTEGER                                            :: n,n1,n2,i
    TYPE(type_sparse_matrix_CSR_dp)                    :: A_CSR

    ! Set up the identity matrix in CSR format
    n = SIZE( x,1)

    ! Allocate memory for the CSR matrix
    CALL allocate_shared_int_0D( A_CSR%m      , A_CSR%wm      )
    CALL allocate_shared_int_0D( A_CSR%n      , A_CSR%wn      )
    CALL allocate_shared_int_0D( A_CSR%nnz_max, A_CSR%wnnz_max)
    CALL allocate_shared_int_0D( A_CSR%nnz    , A_CSR%wnnz    )

    IF (par%master) THEN
      A_CSR%m       = n
      A_CSR%n       = n
      A_CSR%nnz_max = n
      A_CSR%nnz     = n
    END IF
    CALL sync

    CALL allocate_shared_int_1D( A_CSR%m+1, A_CSR%ptr  , A_CSR%wptr  )
    CALL allocate_shared_int_1D( A_CSR%nnz, A_CSR%index, A_CSR%windex)
    CALL allocate_shared_dp_1D(  A_CSR%nnz, A_CSR%val  , A_CSR%wval  )

    ! Fill in values
    CALL partition_list( n, par%i, par%n, n1, n2)
    DO i = n1, n2
      A_CSR%index( i) = i
      A_CSR%ptr(   i) = i
      A_CSR%val(   i) = x( i)
    END DO
    IF (par%master) THEN
      A_CSR%nnz = n
      A_CSR%ptr( n+1) = n+1
    END IF
    CALL sync

    ! Convert to PETSc matrix format
    CALL mat_CSR2petsc( A_CSR, A)

    ! Clean up after yourself
    CALL deallocate_shared( A_CSR%wm      )
    CALL deallocate_shared( A_CSR%wn      )
    CALL deallocate_shared( A_CSR%wnnz_max)
    CALL deallocate_shared( A_CSR%wnnz    )
    CALL deallocate_shared( A_CSR%wptr    )
    CALL deallocate_shared( A_CSR%windex  )
    CALL deallocate_shared( A_CSR%wval    )

  END SUBROUTINE double2petscmat

  SUBROUTINE petscmat_checksum( A, matname)

    IMPLICIT NONE

    ! In/output variables:
    TYPE(tMat),                          INTENT(IN)    :: A
    CHARACTER(LEN=*),                    INTENT(IN)    :: matname

    ! Local variables:
    CHARACTER(LEN=256), PARAMETER                      :: routine_name = 'petscmat_checksum'
    INTEGER                                            :: m, n, istart, iend, i
    INTEGER                                            :: ncols
    INTEGER,  DIMENSION(:    ), ALLOCATABLE            :: cols
    REAL(dp), DIMENSION(:    ), ALLOCATABLE            :: vals
    REAL(dp)                                           :: Amin, Amax, Asum

    ! Add routine to path
    CALL init_routine( routine_name)

    ! First get the number of rows and columns
    CALL MatGetSize( A, m, n, perr)
    CALL MatGetOwnershipRange( A, istart, iend, perr)

    ALLOCATE( cols( n))
    ALLOCATE( vals( n))

    Amin =  1E40_dp
    Amax = -1E40_dp
    Asum = 0._dp

    DO i = istart+1, iend ! +1 because PETSc indexes from 0
      CALL MatGetRow(     A, i-1, ncols, cols, vals, perr)
      Amin = MIN( Amin, MINVAL( vals( 1:ncols)))
      Amax = MAX( Amax, MAXVAL( vals( 1:ncols)))
      Asum =      Asum   + SUM( vals( 1:ncols))
      CALL MatRestoreRow( A, i-1, ncols, cols, vals, perr)
    END DO

    CALL MPI_ALLREDUCE( MPI_IN_PLACE, Amin, 1, MPI_DOUBLE_PRECISION, MPI_MIN, MPI_COMM_WORLD, ierr)
    CALL MPI_ALLREDUCE( MPI_IN_PLACE, Amax, 1, MPI_DOUBLE_PRECISION, MPI_MAX, MPI_COMM_WORLD, ierr)
    CALL MPI_ALLREDUCE( MPI_IN_PLACE, Asum, 1, MPI_DOUBLE_PRECISION, MPI_SUM, MPI_COMM_WORLD, ierr)

    CALL warning( 'CHECKSUM ' // TRIM( matname) // ': MIN = {dp_01}, MAX = {dp_02}, SUM = {dp_03}', dp_01 = Amin, dp_02 = Amax, dp_03 = Asum)

    ! Finalise routine path
    CALL finalise_routine( routine_name)

  END SUBROUTINE petscmat_checksum

! == Matrix-vector multiplication
  SUBROUTINE multiply_PETSc_matrix_with_vector_1D( A, xx, yy)
    ! Multiply a PETSc matrix with a FORTRAN vector: y = A*x

    IMPLICIT NONE

    ! In- and output variables:
    TYPE(tMat),                          INTENT(IN)    :: A
    REAL(dp), DIMENSION(:    ),          INTENT(IN)    :: xx
    REAL(dp), DIMENSION(:    ),          INTENT(OUT)   :: yy

    ! Local variables:
    CHARACTER(LEN=256), PARAMETER                      :: routine_name = 'multiply_PETSc_matrix_with_vector_1D'
    TYPE(PetscInt)                                     :: m, n
    TYPE(tVec)                                         :: x, y

    ! Add routine to path
    CALL init_routine( routine_name)

    ! Safety
    CALL MatGetSize( A, m, n, perr)
    IF (n /= SIZE( xx,1) .OR. m /= SIZE( yy,1)) THEN
      CALL crash('matrix and vector sizes dont match!')
    END IF

    ! Convert Fortran array xx to PETSc vector x
    CALL vec_double2petsc( xx, x)

    ! Set up PETSc vector y for the answer
    CALL VecCreate( PETSC_COMM_WORLD, y, perr)
    CALL VecSetSizes( y, PETSC_DECIDE, m, perr)
    CALL VecSetFromOptions( y, perr)

    ! Compute the matrix-vector product
    CALL MatMult( A, x, y, perr)

    ! Convert PETSc vector y to Fortran array yy
    CALL vec_petsc2double( y, yy)

    ! Clean up after yourself
    CALL VecDestroy( x, perr)
    CALL VecDestroy( y, perr)

    ! Finalise routine path
    CALL finalise_routine( routine_name)

  END SUBROUTINE multiply_PETSc_matrix_with_vector_1D

  SUBROUTINE multiply_PETSc_matrix_with_vector_2D( A, xx, yy)
    ! Multiply a PETSc matrix with a FORTRAN vector: y = A*x

    IMPLICIT NONE

    ! In- and output variables:
    TYPE(tMat),                          INTENT(IN)    :: A
    REAL(dp), DIMENSION(:,:  ),          INTENT(IN)    :: xx
    REAL(dp), DIMENSION(:,:  ),          INTENT(OUT)   :: yy

    ! Local variables:
    CHARACTER(LEN=256), PARAMETER                      :: routine_name = 'multiply_PETSc_matrix_with_vector_2D'
    INTEGER                                            :: m, n, i1, i2, j1, j2, k
    REAL(dp), DIMENSION(:    ), POINTER                ::  xx_1D,  yy_1D
    INTEGER                                            :: wxx_1D, wyy_1D

    ! Add routine to path
    CALL init_routine( routine_name)

    ! Safety
    CALL MatGetSize( A, m, n, perr)

    IF (n /= SIZE( xx,1) .OR. m /= SIZE( yy,1) .OR. SIZE( xx,2) /= SIZE( yy,2)) THEN
      CALL crash('vector sizes dont match!')
    END IF

    ! Allocate shared memory
    CALL allocate_shared_dp_1D( n, xx_1D, wxx_1D)
    CALL allocate_shared_dp_1D( m, yy_1D, wyy_1D)

    CALL partition_list( n, par%i, par%n, i1, i2)
    CALL partition_list( m, par%i, par%n, j1, j2)

    ! Compute each column separately
    DO k = 1, SIZE( xx,2)

      ! Copy this column of x
      xx_1D( i1:i2) = xx( i1:i2,k)
      CALL sync

      ! Compute the matrix-vector product
      CALL multiply_PETSc_matrix_with_vector_1D( A, xx_1D, yy_1D)

      ! Copy the result back
      yy( j1:j2,k) = yy_1D( j1:j2)
      CALL sync

    END DO

    ! Clean up after yourself
    CALL deallocate_shared( wxx_1D)
    CALL deallocate_shared( wyy_1D)

    ! Finalise routine path
    CALL finalise_routine( routine_name)

  END SUBROUTINE multiply_PETSc_matrix_with_vector_2D

  SUBROUTINE mat_A_eq_diag_B_t_A( AA, bb)
    ! A = diag(b) * A
    !
    ! Essentially a wrapper for MatDiagonalScale, so that
    ! the vector can be provided in Fortran format

    IMPLICIT NONE

    ! In/output variables:
    TYPE(tMat),                          INTENT(INOUT) :: AA
    REAL(dp), DIMENSION(:    ),          INTENT(IN)    :: bb

    ! Local variables:
    CHARACTER(LEN=256), PARAMETER                      :: routine_name = 'mat_A_eq_diag_B_t_A'
    TYPE(tVec)                                         :: vbb
    INTEGER                                            :: m,n

    ! Add routine to path
    CALL init_routine( routine_name)

    ! Safety
    CALL MatGetSize( AA, m, n, perr)
    IF (m /= SIZE( bb,1)) THEN
      CALL crash('matrix and vector sizes dont match!')
    END IF

    ! Convert vector from Fortran format to PETSc format
    CALL vec_double2petsc( bb, vbb)

    ! Multiply matrix rows with vector entries
    CALL MatDiagonalScale( AA, vbb, PETSC_NULL_VEC, perr)

    ! Clean up after yourself
    CALL VecDestroy( vbb, perr)

    ! Finalise routine path
    CALL finalise_routine( routine_name)

  END SUBROUTINE mat_A_eq_diag_B_t_A

  SUBROUTINE mat_A_eq_B_p_diagC_t_D( BB, cc, DD, AA)
    ! A = B + diag(C) * D
    !
    ! Assumes A has not been allocated yet
    ! Vector cc is provided in Fortran format

    IMPLICIT NONE

    ! In/output variables:
    TYPE(tMat),                          INTENT(IN)    :: BB
    REAL(dp), DIMENSION(:    ),          INTENT(IN)    :: cc
    TYPE(tMat),                          INTENT(IN)    :: DD
    TYPE(tMat),                          INTENT(OUT)   :: AA

    ! Local variables:
    CHARACTER(LEN=256), PARAMETER                      :: routine_name = 'mat_A_eq_B_p_diagC_t_D'
    INTEGER                                            :: mb,nb,md,nd,mc
    TYPE(tMat)                                         :: ccDD

    ! Add routine to path
    CALL init_routine( routine_name)

    ! Safety
    CALL MatGetSize( BB, mb, nb, perr)
    CALL MatGetSize( DD, md, nd, perr)
    mc = SIZE( cc,1)
    IF (mb /= md .OR. nb /= nd .OR. mb /= mc) THEN
      CALL crash('matrix and vector sizes dont match!')
    END IF

    ! ccDD = diag(C) * D
    CALL MatDuplicate( DD, MAT_COPY_VALUES, ccDD, perr)
    CALL mat_A_eq_diag_B_t_A( ccDD, cc)

    ! AA = BB
    CALL MatDuplicate( BB, MAT_COPY_VALUES, AA, perr)

    ! AA = BB + ccDD
    CALL MatAXPY( AA, 1._dp, ccDD, DIFFERENT_NONZERO_PATTERN, perr)

    ! Clean up after yourself
    CALL MatDestroy( ccDD, perr)

    ! Finalise routine path
    CALL finalise_routine( routine_name)

  END SUBROUTINE mat_A_eq_B_p_diagC_t_D

END MODULE petsc_module
